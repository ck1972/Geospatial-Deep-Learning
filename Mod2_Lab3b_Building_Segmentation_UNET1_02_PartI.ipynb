{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Semantic Segmentation of Aerial Imagery for Building Footprint Extraction Using U-Net: Part 1**"
      ],
      "metadata": {
        "id": "Gph9S2d3jEb6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install required packages\n",
        "Before starting the building footprint extraction workflow using U-Net, we need to install a few essential Python libraries that are not included by default in Google Colab. These installations are done using the pip package manager, and the --quiet flag suppresses verbose output to keep the notebook clean."
      ],
      "metadata": {
        "id": "XXXzN5a6CsX9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OHRHvi8ki5ru",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install torchgeo --quiet\n",
        "!pip install buildingregulariser --quiet\n",
        "!pip install torchsummary --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import required libraries"
      ],
      "metadata": {
        "id": "UM83pkHpjQfL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import geopandas as gpd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchsummary import summary\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import rasterio\n",
        "from rasterio import features\n",
        "import shapely.geometry as shp_geom\n",
        "from buildingregulariser import regularize_geodataframe\n",
        "from sklearn.metrics import f1_score, jaccard_score\n",
        "import scipy.ndimage as nd"
      ],
      "metadata": {
        "id": "VkW_y_yVjQ8H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mount Google Drive and set the working directory"
      ],
      "metadata": {
        "id": "lkYFBammjUNd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "os.chdir('/content/drive/MyDrive/Chit_TR9207')"
      ],
      "metadata": {
        "id": "Vu6iS0jajUoZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load the raster image and rasterize building footprints"
      ],
      "metadata": {
        "id": "mLr49-bljYe0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load raster and geojson\n",
        "\n",
        "# Define the path to the aerial image (GeoTIFF file)\n",
        "img_path = 'TR9207a.tif'\n",
        "\n",
        "# Open the raster image using rasterio\n",
        "with rasterio.open(img_path) as src:\n",
        "    transform = src.transform               # Get the affine transformation (pixel-to-coordinate mapping)\n",
        "    height, width = src.height, src.width   # Extract image dimensions (rows and columns)\n",
        "    raster_crs = src.crs                    # Get the coordinate reference system (CRS) of the raster\n",
        "    image = src.read()                      # Read the raster data as a NumPy array (shape: [bands, height, width])\n",
        "\n",
        "# Read building footprint polygons from the GeoJSON file into a GeoDataFrame\n",
        "gdf = gpd.read_file('Bld_DSG_TR9207.geojson')\n",
        "\n",
        "# Rasterize the vector geometries into a binary mask\n",
        "# Each building polygon is assigned a value of 1, and the background is filled with 0\n",
        "# Rasterize the vector geometries into a binary mask\n",
        "# Each building polygon is assigned a value of 1, and the background is filled with 0\n",
        "mask = features.rasterize(\n",
        "    ((geom, 1) for geom in gdf.geometry),   # Generator of (geometry, value) pairs\n",
        "    out_shape=(height, width),             # Output mask will match the raster's dimensions\n",
        "    transform=transform,                   # Use the same affine transform as the raster\n",
        "    fill=0,                                # Set background (non-building) pixels to 0\n",
        "    dtype=np.uint8                         # Use 8-bit unsigned integer type for the mask\n",
        ")"
      ],
      "metadata": {
        "id": "VYEA-aAEjY3p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define the custom dataset for patch extraction"
      ],
      "metadata": {
        "id": "NNrYS3InjgA2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a custom PyTorch dataset for aerial image building segmentation\n",
        "class AerialBuildingDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, image, mask):\n",
        "        self.image = image                  # Multi-band aerial image (NumPy array of shape: [bands, height, width])\n",
        "        self.mask = mask                    # Corresponding binary mask (NumPy array of shape: [height, width])\n",
        "        self.patch_size = 256               # Define the patch size (e.g., 256x256 pixels)\n",
        "        self.bands, self.height, self.width = image.shape  # Extract dimensions from the image\n",
        "\n",
        "        # Generate top-left coordinates for each non-overlapping patch in the image\n",
        "        self.patch_coords = [\n",
        "            (i, j)\n",
        "            for i in range(0, self.height - self.patch_size + 1, self.patch_size)\n",
        "            for j in range(0, self.width - self.patch_size + 1, self.patch_size)\n",
        "        ]\n",
        "\n",
        "    def __len__(self):\n",
        "        # Return the total number of patches available\n",
        "        return len(self.patch_coords)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Get top-left corner (i, j) of the patch based on index\n",
        "        i, j = self.patch_coords[idx]\n",
        "\n",
        "        # Extract image patch and corresponding mask patch\n",
        "        img_patch = self.image[:, i:i+self.patch_size, j:j+self.patch_size]   # Shape: [bands, patch_size, patch_size]\n",
        "        mask_patch = self.mask[i:i+self.patch_size, j:j+self.patch_size]      # Shape: [patch_size, patch_size]\n",
        "\n",
        "        # Convert both to PyTorch tensors\n",
        "        img_patch = torch.tensor(img_patch, dtype=torch.float32)              # Float tensor for image\n",
        "        mask_patch = torch.tensor(mask_patch, dtype=torch.long)              # Long tensor for mask (for classification)\n",
        "\n",
        "        return img_patch, mask_patch"
      ],
      "metadata": {
        "id": "6kvjt95ZjgX-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create the dataset and data loaders"
      ],
      "metadata": {
        "id": "XjrIb723BoyT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an instance of the custom dataset using the full image and mask\n",
        "dataset = AerialBuildingDataset(image, mask)\n",
        "\n",
        "# Define the training set size as 80% of the total dataset\n",
        "train_size = int(0.8 * len(dataset))\n",
        "\n",
        "# Define the test set size as the remaining 20%\n",
        "test_size = len(dataset) - train_size\n",
        "\n",
        "# Randomly split the dataset into training and testing subsets\n",
        "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
        "\n",
        "# Create a DataLoader for the training set\n",
        "# - batch_size=4: feed 4 patches at a time\n",
        "# - shuffle=True: randomly shuffle data each epoch to improve generalization\n",
        "# - drop_last=True: drop the last batch if it's smaller than the batch size\n",
        "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, drop_last=True)\n",
        "\n",
        "# Create a DataLoader for the test set\n",
        "# - shuffle=False: maintain original order for evaluation consistency\n",
        "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False)"
      ],
      "metadata": {
        "id": "1lnAPzNlBpK5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}